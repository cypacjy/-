# 神经网络与深度学习笔记

## 概论

人工神经网络

定义：

是从微观结构与功能上模拟人脑神经系统而建立的一类
模型，是模拟人的智能的一条途径。

特点：

网络的信息处理，由神经元间的相互作用实现，具有并行处理的特点；
知识与信息的存储，表现为神经元间分布式的物理联系；
网络的学习和识别，决定于神经元联接权系数的动态演化过程；
具有联想记忆特性

机器学习发展：

![](C:\Users\chenjy\Desktop\新建文件夹%20(3)\2023-03-20-00-34-04-image.png)

人工智能应用级别：弱人工智能，强人工智能，超人工智能

人工智能基石：数据，算法，计算

人工智能关键技术：计算机视觉，自然语言处理，机器人，语音识别，机器学习，图像识别



# 线性分类

## 1.线性回归：

 $问题描述：针对样本(x^i,y^i),i=1,2,3...n.其中x为m维数据，y为1维数据。\\构造代价函数J(\theta)=\dfrac{1}{2}\sum_{i=1}^n(y^i-h_\theta(x^i))^2,其中h_\theta（x^i）=\theta^Tx,\theta为m维列向量。\\目标：寻找超平面参数\theta,使J(\theta)最小.$

#### 求解方法：

$1:直接求解，即\theta=(X^TX)^{-1}X^Ty$

$2:优化方法->梯度下降法$

### 2.线性分类（以二分类为例）

$问题描述：针对样本(x^i,y^i),i=1,2,3...n.其中x为m维数据，y为0或1。\\构造代价函数J(\theta)=\dfrac{1}{2}\sum_{i=1}^n(y^i-h_\theta(x^i))^2,其中h_\theta（x^i）=\dfrac{1}{1+e^{-z}},z=\theta_1x_1+\theta_2x_2+...+\theta_0。\\目标：寻找参数\theta,使J(\theta)最小.$

$求解方法->梯度下降法\\$

$构造迭代序列\theta_1,\theta_2,\theta_3,...求解：\\其中\theta_1任取，\theta_{i+1}=\theta_i+\Delta\theta_i,i=1,2,3,4...\\\Delta\theta_i=-\alpha\dfrac{dJ(\theta_i)}{d\theta_i},\\\dfrac{dJ(\theta_i)}{d\theta_i}=\sum_{i=1}^n(y^i-h_\theta(x^i))(-\dfrac{\partial h_\theta(x^i)}{\partial \theta})\\\dfrac{\partial h_\theta(x^i)}{\partial \theta}=h_\theta(1-h_\theta)x^i$

### 3.对数回归

思路：转化维条件概率角度求解

$代价函数：J(\theta)=-\sum(y^ilog(h_\theta(x^i))+(1-y^i)log(1-h_\theta(x^i)))$

求解方法：梯度下降法

$\Delta_\theta J(\theta)=\sum_i x^i(h_\theta(x^i)-y^i)$



### 4.线性多分类

$代价函数：J(\theta)=-[\sum_{i=1}^n\sum_{k=1}^k\log\dfrac{e^{\theta^Tx^i}}{\sum_{j=1}^ke^{\theta^Tx^i}}]$

求解方法：梯度下降法

$\Delta_{\theta^k}=-\sum_{i=1}^m[x^i(1\{y^i=k\}-p(y^i=k|x^i;\theta)]$

# 感知机

## 1：神经元模型

1:基本模型

spiking模型：

$对于每个时刻的脉冲按加权求和达到阈值输出脉冲信号实例：脉冲神经网络SNN$

integrate-and-fire模型：

$仿照生物电信号放电过程，神经元在电流刺激下达到阈值电位就发出动作信号，否则无输出$

mp模型：$从神经元的模型中可以得出神经元的输出y=f(\sum_1^n\omega_ix_i-\theta),\\其中\theta为神经元的激活阈值，\\函数f(⋅)被是激活函数。函数f(⋅)可用阶跃方程表示，大于阈值激活；否则则抑制。但阶跃函数不光滑，不连续，不可导，\\因此通常用sigmoid函数来表示函数f(⋅)。$

2：作用函数




$1:非对称sigmoid函数：f(x)=\dfrac{1}{1+e^{-\beta x}}\\2:对称sigmoid函数：f(x)=\dfrac{1}{1+e^{-\beta x}}\\3:对称性阶跃函数f(x)=\left\{
\begin{aligned}
+1 & , & x\geqslant0 \\
-1 & , & x<0
\end{aligned}
\right.$

## 2：单层感知机模型

用途：解决线性分类问题

缺点：难以解决xor问题

$模型：y=f(x)=sign(\omega^Tx)\\其中sign(x)=\left\{
\begin{aligned}
+1 & , & x\geqslant0 \\
-1 & , & x<0
\end{aligned}
\right.$

算法：

$1.赋初值\omega_0,数据序号i=1，迭代次数k = 0。\\2.选择数据点(x^i,y^i)\\3.判断该数据点是否为当前模型的误分类点，即判断若y^i(\omega^Tx^i)\ge 0,则更新权值:\omega_{i+1}=\omega_i+\eta y^ix^i,\\4.转到2，直到训练集中没有误分类点$

$损失函数：L(\omega)=-\dfrac{1}{||\omega||}\sum y^i(\omega^Tx)$

## 3：多层感知机模型

多层感知器网络，有如下定理：

> 定理1 若隐层节点（单元）可任意设置，用三层阈值节点的网络，可以实现任意的二值逻辑函数。
> 定理2 若隐层节点（单元）可任意设置，用三层S型非线性特性节点的网络，可以一致逼近紧集上的连续函数或按 范数逼近紧集上的平方可积函数。

例：

<img src="file:///C:/Users/chenjy/AppData/Roaming/marktext/images/2023-03-20-03-06-59-image.png" title="" alt="" width="188">

### 4：反馈算法

思路：

1：随机初始化多层感知机网络参数

2：针对随机化的网络参数，求出网络输出值

3：针对描述网络输出值与实际值的关系的损失函数计算关于参数的梯度

4：使用求得的梯度对参数更新

5：使用更新的参数求出网络输出值

6：重复3-5步，直至网络达到要求精度



损失函数可增加正则项



优点：

1. 学习完全自主；

2.  n可逼近任意非线性函数

缺点：

1. 算法非全局收敛

2. 收敛速度慢

3. 学习速率α选择

4. 神经网络如何设计(几层？节点数？）

例子：对sin(x)进行训练

```python
# -*- coding: utf-8 -*-
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
X=torch.linspace(0, 6*np.pi,10000)
X=torch.unsqueeze(X, dim=1)
Y=orch.sin(X)
class multiplayer(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear1=nn.Linear(1, 100)
        self.linear2=nn.Linear(100, 1)
        
    def forward(self,X):
        X=self.linear1(X)
        X=torch.sigmoid(X)
        X=self.linear2(X)
        return X
net=multiplayer()
opt=torch.optim.Adam(net.parameters(),lr=0.005)
#opt = torch.optim.SGD(net.parameters(),lr=0.5)
#设置学习率为0.5，用随机梯度下降发优化神经网络的参数
lossfunc = torch.nn.MSELoss()
#设置损失函数为均方损失函数，用来计算每次的误差
losses=[]
for t in range(10000):
#进行100次的优化
    prediction = net(X)
	#得到预测值
    loss=lossfunc(prediction,Y)
    opt.zero_grad()
	#梯度清零
    loss.backward()
	#反向传播
    opt.step()
	#梯度优化

```

# 性能优化

### 性能优化算法比较

> 动量法：在梯度下降法中将下降梯度增加惯性项
> 
> 自适应梯度法：在梯度下降法中，保持梯度方向不变，按迭代次数依次降低梯度大小以减小震荡
> 
> RMSProp算法：在自适应梯度法加入超参数控制梯度大小衰减速率
> 
> adam算法：在RMSProp算法中保留历史梯度的指数衰减平均值

## 1：动量法

![](C:\Users\chenjy\AppData\Roaming\marktext\images\2023-03-20-02-55-31-image.png)

## 2：自适应梯度法

![](C:\Users\chenjy\AppData\Roaming\marktext\images\2023-03-20-02-55-44-image.png)

![](C:\Users\chenjy\AppData\Roaming\marktext\images\2023-03-20-02-55-53-image.png)

![](C:\Users\chenjy\AppData\Roaming\marktext\images\2023-03-20-02-56-02-image.png)
